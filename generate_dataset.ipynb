{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# apply masks & generate pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: No IPv4 address found on anpi1 !\n",
      "WARNING: No IPv4 address found on anpi0 !\n",
      "WARNING: more No IPv4 address found on en3 !\n",
      "WARNING: No IPv4 address found on bridge0 !\n",
      "WARNING: No IPv4 address found on awdl0 !\n",
      "WARNING: more No IPv4 address found on llw0 !\n",
      "WARNING: No IPv4 address found on en2 !\n",
      "WARNING: No IPv4 address found on ap1 !\n",
      "WARNING: more No IPv4 address found on bridge0 !\n",
      "/Users/tianh/anaconda3/lib/python3.11/site-packages/scapy/layers/ipsec.py:512: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  cipher=algorithms.TripleDES,\n",
      "/Users/tianh/anaconda3/lib/python3.11/site-packages/scapy/layers/ipsec.py:516: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  cipher=algorithms.TripleDES,\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from scapy.all import rdpcap, IP, TCP, UDP, PcapReader, DNS\n",
    "import pickle\n",
    "def bytes2bits(x):\n",
    "    return ''.join(f'{byte:08b}' for byte in x)\n",
    "def bits2bytes(x):\n",
    "    return bytes(int(x[i:i+8], 2) for i in range(0, len(x), 8))\n",
    "def mask(bits, start, end):\n",
    "    return bits[:start] + '0'*(end-start) + bits[end:]\n",
    "def bits2ints(b):\n",
    "    b = b.zfill((len(b) + 7) // 8 * 8)\n",
    "    return [int(b[i:i+8], 2) for i in range(0, len(b), 8)]\n",
    "\n",
    "def mask_ip_header(packet):\n",
    "    if IP not in packet:\n",
    "        return ''\n",
    "    \n",
    "    ip_header = bytes(packet[IP])\n",
    "    ip_header_bits = bytes2bits(ip_header)\n",
    "    U = int(ip_header_bits[4:8],2)\n",
    "    ip_header_bits = ip_header_bits[:(U*32)]\n",
    "\n",
    "    ip_header_bits = mask(ip_header_bits, 32, 48) # identification\n",
    "    ip_header_bits = mask(ip_header_bits, 80, 96) # checksum\n",
    "    ip_header_bits = mask(ip_header_bits, 96, 128) # src ip\n",
    "    ip_header_bits = mask(ip_header_bits, 128, 160) # dst ip\n",
    "    return bits2bytes(ip_header_bits)\n",
    "\n",
    "def mask_tcpudp_header(packet):\n",
    "    if TCP in packet:\n",
    "        tcp_len = packet[TCP].dataofs\n",
    "        header = bytes(packet[TCP])[:(tcp_len*4)]\n",
    "        header_bits = bytes2bits(header)\n",
    "    elif UDP in packet:\n",
    "        header = bytes(packet[UDP])[:8]\n",
    "        header_bits = bytes2bits(header)\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "    header_bits = mask(header_bits, 0, 16) # src port\n",
    "    header_bits = mask(header_bits, 16, 32) # dst port\n",
    "    return bits2bytes(header_bits)\n",
    "\n",
    "\n",
    "def preprocess_packet(packet, exclude_payload=False, payload_maxlen=1500):\n",
    "    if TCP in packet:\n",
    "        payload = bytes(packet[TCP].payload)\n",
    "    elif UDP in packet:\n",
    "        payload = bytes(packet[UDP].payload)\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "    ip_header = mask_ip_header(packet)\n",
    "    tcpudp_header = mask_tcpudp_header(packet)\n",
    "    if len(ip_header) == 0 or len(tcpudp_header) == 0:\n",
    "        return ''\n",
    "    \n",
    "    out = [list(ip_header), list(tcpudp_header)]\n",
    "    if len(payload) == 0:\n",
    "        if exclude_payload:\n",
    "            return out\n",
    "        else:\n",
    "            return ''\n",
    "    else:\n",
    "        if exclude_payload:\n",
    "            return ''\n",
    "        else:\n",
    "            payload = list(payload)\n",
    "            payload = payload[:min(payload_maxlen, len(payload))]\n",
    "            out.append(payload)\n",
    "            return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(dataset_dir, out_dir, exclude_payload=False):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    DNS.dissect = lambda self, s: None\n",
    "    for root, dirs, files in os.walk(dataset_dir):  # subdir level\n",
    "        for file in files: \n",
    "            if file.endswith('.pcap'):\n",
    "                file_path = os.path.join(root, file)  \n",
    "                label = file_path.split('/')[-2]\n",
    "                out_file = os.path.join(out_dir, label+'.pkl')\n",
    "                \n",
    "                print(file_path)\n",
    "                i = 0\n",
    "                packets = rdpcap(file_path)\n",
    "                for packet in packets:\n",
    "                    out = preprocess_packet(packet, exclude_payload=exclude_payload)\n",
    "                    if len(out) == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    if exclude_payload:\n",
    "                        assert(len(out) == 2)\n",
    "                    else:\n",
    "                        assert(len(out[2]) > 0)\n",
    "                    i += 1\n",
    "                    with open(out_file, \"ab\") as f: \n",
    "                        pickle.dump(out, f)\n",
    "                print(f'Extracted {i} out of {len(packets)} packets')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../datasets/raw/D2/twitter/twitter_1.pcap\n",
      "Extracted 125186 out of 356192 packets\n",
      "../datasets/raw/D2/ted/TED_2.pcap\n",
      "Extracted 35422 out of 78244 packets\n",
      "../datasets/raw/D2/ted/TED_1.pcap\n",
      "Extracted 133641 out of 353894 packets\n",
      "../datasets/raw/D2/amazon/amazon_1.pcap\n",
      "Extracted 104148 out of 250460 packets\n",
      "../datasets/raw/D2/baidu/baidu_2.pcap\n",
      "Extracted 12116 out of 23688 packets\n",
      "../datasets/raw/D2/baidu/baidu_1.pcap\n",
      "Extracted 167282 out of 294552 packets\n",
      "../datasets/raw/D2/youku/youku_1.pcap\n",
      "Extracted 82031 out of 221172 packets\n",
      "../datasets/raw/D2/douban/douban_1.pcap\n",
      "Extracted 76464 out of 215897 packets\n",
      "../datasets/raw/D2/google/google_2.pcap\n",
      "Extracted 9017 out of 20460 packets\n",
      "../datasets/raw/D2/google/google_1.pcap\n",
      "Extracted 52421 out of 112995 packets\n",
      "../datasets/raw/D2/bing/bing_2.pcap\n",
      "Extracted 20467 out of 55395 packets\n",
      "../datasets/raw/D2/bing/bing_1.pcap\n",
      "Extracted 73301 out of 200663 packets\n",
      "../datasets/raw/D2/youtube/youtube_1.pcap\n",
      "Extracted 203127 out of 519091 packets\n",
      "../datasets/raw/D2/facebook/facebook_1.pcap\n",
      "Extracted 156127 out of 369361 packets\n",
      "../datasets/raw/D2/facebook/facebook_2.pcap\n",
      "Extracted 28298 out of 65436 packets\n",
      "../datasets/raw/D2/weibo/weibo_1.pcap\n",
      "Extracted 172385 out of 495974 packets\n",
      "../datasets/raw/D2/imdb/imdb_2.pcap\n",
      "Extracted 33894 out of 77307 packets\n",
      "../datasets/raw/D2/imdb/imdb_1.pcap\n",
      "Extracted 107477 out of 317850 packets\n",
      "../datasets/raw/D2/tieba/tieba_1.pcap\n",
      "Extracted 95092 out of 221049 packets\n",
      "../datasets/raw/D2/reddit/reddit_1.pcap\n",
      "Extracted 137154 out of 340920 packets\n",
      "../datasets/raw/D2/reddit/reddit_2.pcap\n",
      "Extracted 23762 out of 53017 packets\n",
      "../datasets/raw/D2/taobao/taobao_1.pcap\n",
      "Extracted 122451 out of 350890 packets\n",
      "../datasets/raw/D2/iqiyi/iqiyi_1.pcap\n",
      "Extracted 89362 out of 321825 packets\n",
      "../datasets/raw/D2/jd/JD_1.pcap\n",
      "Extracted 89161 out of 240306 packets\n",
      "../datasets/raw/D2/instagram/instagram_1.pcap\n",
      "Extracted 82948 out of 180230 packets\n",
      "../datasets/raw/D2/instagram/instagram_2.pcap\n",
      "Extracted 25468 out of 53340 packets\n",
      "../datasets/raw/D2/netease/NeteaseMusic_1.pcap\n",
      "Extracted 88987 out of 216322 packets\n",
      "../datasets/raw/D2/netease/NeteaseMusic_2.pcap\n",
      "Extracted 18725 out of 48585 packets\n",
      "../datasets/raw/D2/qq/qqmail_1.pcap\n",
      "Extracted 91279 out of 183574 packets\n",
      "../datasets/raw/D2/qq/qqmail_2.pcap\n",
      "Extracted 8703 out of 17343 packets\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = '../datasets/raw/D2'\n",
    "out_dir = '../datasets/processed/D2_nopayload'\n",
    "\n",
    "process_dataset(dataset_dir, out_dir, exclude_payload=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# more processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = '../datasets/processed/D2'\n",
    "out_dir = '../datasets/processed/D2_half'\n",
    "\n",
    "import pickle, os\n",
    "os.makedirs(out_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading tieba.pkl\n",
      "Extracting 62876 packets from tieba.pkl\n",
      "Finished reading instagram.pkl\n",
      "Extracting 62568 packets from instagram.pkl\n",
      "Finished reading weibo.pkl\n",
      "Extracting 161742 packets from weibo.pkl\n",
      "Finished reading imdb.pkl\n",
      "Extracting 126816 packets from imdb.pkl\n",
      "Finished reading jd.pkl\n",
      "Extracting 75494 packets from jd.pkl\n",
      "Finished reading baidu.pkl\n",
      "Extracting 69077 packets from baidu.pkl\n",
      "Finished reading taobao.pkl\n",
      "Extracting 113999 packets from taobao.pkl\n",
      "Finished reading reddit.pkl\n",
      "Extracting 116287 packets from reddit.pkl\n",
      "Finished reading netease.pkl\n",
      "Extracting 78369 packets from netease.pkl\n",
      "Finished reading youku.pkl\n",
      "Extracting 69522 packets from youku.pkl\n",
      "Finished reading qq.pkl\n",
      "Extracting 49862 packets from qq.pkl\n",
      "Finished reading twitter.pkl\n",
      "Extracting 115497 packets from twitter.pkl\n",
      "Finished reading google.pkl\n",
      "Extracting 35861 packets from google.pkl\n",
      "Finished reading ted.pkl\n",
      "Extracting 131454 packets from ted.pkl\n",
      "Finished reading iqiyi.pkl\n",
      "Extracting 115766 packets from iqiyi.pkl\n",
      "Finished reading douban.pkl\n",
      "Extracting 69611 packets from douban.pkl\n",
      "Finished reading youtube.pkl\n",
      "Extracting 157957 packets from youtube.pkl\n",
      "Finished reading bing.pkl\n",
      "Extracting 80960 packets from bing.pkl\n",
      "Finished reading facebook.pkl\n",
      "Extracting 125171 packets from facebook.pkl\n",
      "Finished reading amazon.pkl\n",
      "Extracting 73123 packets from amazon.pkl\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "subset_pct = 0.5\n",
    "num_packets = 0\n",
    "for file in os.listdir(dataset_dir):  # subdir level\n",
    "    if file.endswith('.pkl'):\n",
    "        lines = []\n",
    "        with open(os.path.join(dataset_dir, file), 'rb') as f:\n",
    "            try:\n",
    "                while True:\n",
    "                    lines.append(pickle.load(f))\n",
    "            except EOFError:\n",
    "                print(\"Finished reading \" + file)\n",
    "\n",
    "        subset = random.sample(lines, int(len(lines)*subset_pct))\n",
    "        num_packets += len(subset)\n",
    "        print(f\"Extracting {len(subset)} packets from \" + file)\n",
    "        with open(os.path.join(out_dir, file), \"ab\") as f: \n",
    "            for packet in subset:\n",
    "                pickle.dump(packet, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1255178"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_packets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle\n",
    "for root, dirs, files in os.walk('../datasets/processed/D1/'):  # subdir level\n",
    "    for file in files: \n",
    "        X, Y = [], []\n",
    "        if file.endswith('.pkl'):\n",
    "            label = file[:-4]\n",
    "            file_name = os.path.join(root, file)\n",
    "            with open(file_name, 'rb') as f:\n",
    "                try:\n",
    "                    while True:\n",
    "                        packet = pickle.load(f)\n",
    "                        assert(len(packet)==3)\n",
    "                        X.append(packet)\n",
    "                        Y.append(label)\n",
    "                except EOFError:\n",
    "                    print(\"Finished reading \" + file_name)\n",
    "            \n",
    "            lens = [len(segment_array(x)) for x in X]\n",
    "            print(max(lens)*8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "compress_dir = '../datasets/processed/D1_nopayload'\n",
    "out_name = '../datasets/processed/D1_nopayload'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/tianh/Desktop/EBSNN/datasets/processed/D1_nopayload.tar.gz'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "shutil.make_archive(out_name, 'gztar', compress_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

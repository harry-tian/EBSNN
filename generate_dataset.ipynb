{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# apply masks & generate pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scapy.all import rdpcap, IP, TCP, UDP, DNS\n",
    "import pickle\n",
    "from utils import mask_ip_header, mask_tcpudp_header\n",
    "\n",
    "def preprocess_packet(packet, exclude_payload=False, payload_maxlen=1500):\n",
    "    if TCP in packet:\n",
    "        payload = bytes(packet[TCP].payload)\n",
    "    elif UDP in packet:\n",
    "        payload = bytes(packet[UDP].payload)\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "    ip_header = mask_ip_header(packet)\n",
    "    tcpudp_header = mask_tcpudp_header(packet)\n",
    "    if len(ip_header) == 0 or len(tcpudp_header) == 0:\n",
    "        return ''\n",
    "    \n",
    "    out = [list(ip_header), list(tcpudp_header)]\n",
    "    if len(payload) == 0:\n",
    "        if exclude_payload:\n",
    "            return out\n",
    "        else:\n",
    "            return ''\n",
    "    else:\n",
    "        if exclude_payload:\n",
    "            return ''\n",
    "        else:\n",
    "            payload = list(payload)\n",
    "            payload = payload[:min(payload_maxlen, len(payload))]\n",
    "            out.append(payload)\n",
    "            return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(dataset_dir, out_dir, exclude_payload=False):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    DNS.dissect = lambda self, s: None\n",
    "    for root, dirs, files in os.walk(dataset_dir):  # subdir level\n",
    "        for file in files: \n",
    "            if file.endswith('.pcap'):\n",
    "                file_path = os.path.join(root, file)  \n",
    "                label = file_path.split('/')[-2]\n",
    "                out_file = os.path.join(out_dir, label+'.pkl')\n",
    "                \n",
    "                print(file_path)\n",
    "                i = 0\n",
    "                packets = rdpcap(file_path)\n",
    "                for packet in packets:\n",
    "                    packet = preprocess_packet(packet, exclude_payload=exclude_payload)\n",
    "                    if len(packet) == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    if exclude_payload:\n",
    "                        assert(len(packet) == 2)\n",
    "                    else:\n",
    "                        assert(len(packet[2]) > 0)\n",
    "                    i += 1\n",
    "                    with open(out_file, \"ab\") as f: \n",
    "                        pickle.dump(packet, f)\n",
    "                print(f'Extracted {i} out of {len(packets)} packets')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/raw/D3/appleStocks/appleStocks.pcap\n",
      "Extracted 14910 out of 19167 packets\n",
      "../data/raw/D3/steam/steam_update_full.pcap\n",
      "Extracted 143676 out of 234558 packets\n",
      "../data/raw/D3/steam/steam_browsing_download.pcap\n",
      "Extracted 2198 out of 3689 packets\n",
      "../data/raw/D3/minecraft/Minecraft1.21_local_server.pcap\n",
      "Extracted 25225 out of 40798 packets\n",
      "../data/raw/D3/minecraft/Minecraft1.21_hypixel.pcap\n",
      "Extracted 62559 out of 76313 packets\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = '../data/raw/D3'\n",
    "out_dir = '../data/processed/D3'\n",
    "\n",
    "process_dataset(dataset_dir, out_dir, exclude_payload=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# subsettting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = '../data/processed/D1g'\n",
    "out_dir = '../data/processed/D1g_half'\n",
    "\n",
    "import pickle, os\n",
    "os.makedirs(out_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading kugou.pkl\n",
      "Extracting 85083 packets from kugou.pkl\n",
      "Finished reading tudou.pkl\n",
      "Extracting 44860 packets from tudou.pkl\n",
      "Finished reading spotify.pkl\n",
      "Extracting 4911 packets from spotify.pkl\n",
      "Finished reading vimeo.pkl\n",
      "Extracting 17612 packets from vimeo.pkl\n",
      "Finished reading aimchat.pkl\n",
      "Extracting 9312 packets from aimchat.pkl\n",
      "Finished reading thunder.pkl\n",
      "Extracting 32162 packets from thunder.pkl\n",
      "Finished reading weibo.pkl\n",
      "Extracting 24898 packets from weibo.pkl\n",
      "Finished reading sohu.pkl\n",
      "Extracting 19507 packets from sohu.pkl\n",
      "Finished reading steam.pkl\n",
      "Extracting 72937 packets from steam.pkl\n",
      "Finished reading minecraft.pkl\n",
      "Extracting 43892 packets from minecraft.pkl\n",
      "Finished reading voipbuster.pkl\n",
      "Extracting 179895 packets from voipbuster.pkl\n",
      "Finished reading gmail.pkl\n",
      "Extracting 5501 packets from gmail.pkl\n",
      "Finished reading jd.pkl\n",
      "Extracting 9743 packets from jd.pkl\n",
      "Finished reading skype.pkl\n",
      "Extracting 294419 packets from skype.pkl\n",
      "Finished reading baidu.pkl\n",
      "Extracting 18182 packets from baidu.pkl\n",
      "Finished reading yahoomail.pkl\n",
      "Extracting 19474 packets from yahoomail.pkl\n",
      "Finished reading netflix.pkl\n",
      "Extracting 27585 packets from netflix.pkl\n",
      "Finished reading taobao.pkl\n",
      "Extracting 15568 packets from taobao.pkl\n",
      "Finished reading itunes.pkl\n",
      "Extracting 9033 packets from itunes.pkl\n",
      "Finished reading youku.pkl\n",
      "Extracting 49958 packets from youku.pkl\n",
      "Finished reading qq.pkl\n",
      "Extracting 7022 packets from qq.pkl\n",
      "Finished reading twitter.pkl\n",
      "Extracting 4656 packets from twitter.pkl\n",
      "Finished reading pplive.pkl\n",
      "Extracting 90556 packets from pplive.pkl\n",
      "Finished reading google.pkl\n",
      "Extracting 3381 packets from google.pkl\n",
      "Finished reading sinauc.pkl\n",
      "Extracting 77975 packets from sinauc.pkl\n",
      "Finished reading MS-Exchange.pkl\n",
      "Extracting 860 packets from MS-Exchange.pkl\n",
      "Finished reading youtube.pkl\n",
      "Extracting 20387 packets from youtube.pkl\n",
      "Finished reading mssql.pkl\n",
      "Extracting 10441 packets from mssql.pkl\n",
      "Finished reading facebook.pkl\n",
      "Extracting 122496 packets from facebook.pkl\n",
      "Finished reading cloudmusic.pkl\n",
      "Extracting 42436 packets from cloudmusic.pkl\n",
      "Finished reading amazon.pkl\n",
      "Extracting 7265 packets from amazon.pkl\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "subset_pct = 0.5\n",
    "num_packets = 0\n",
    "for file in os.listdir(dataset_dir):  # subdir level\n",
    "    if file.endswith('.pkl'):\n",
    "        lines = []\n",
    "        with open(os.path.join(dataset_dir, file), 'rb') as f:\n",
    "            try:\n",
    "                while True:\n",
    "                    lines.append(pickle.load(f))\n",
    "            except EOFError:\n",
    "                print(\"Finished reading \" + file)\n",
    "\n",
    "        subset = random.sample(lines, int(len(lines)*subset_pct))\n",
    "        num_packets += len(subset)\n",
    "        print(f\"Extracting {len(subset)} packets from \" + file)\n",
    "        with open(os.path.join(out_dir, file), \"ab\") as f: \n",
    "            for packet in subset:\n",
    "                pickle.dump(packet, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle\n",
    "from scapy.all import rdpcap, IP, TCP, UDP, DNS\n",
    "from collections import defaultdict\n",
    "from utils import mask_ip_header, mask_tcpudp_header\n",
    "\n",
    "def extract_flows(packets, threshold=50, exclude_payload=False):\n",
    "    tcp_flows = defaultdict(list)\n",
    "    udp_flows = defaultdict(list)\n",
    "    for pkt in packets:\n",
    "        if IP in pkt:\n",
    "            if TCP in pkt:\n",
    "                if len(bytes(pkt[TCP].payload)) > 0:\n",
    "                    flow_key = (pkt[IP].src, pkt[IP].dst, pkt[TCP].sport, pkt[TCP].dport,pkt[TCP].name )\n",
    "                    tcp_flows[flow_key].append(pkt)\n",
    "            elif UDP in pkt:\n",
    "                if len(bytes(pkt[UDP].payload)) > 0:\n",
    "                    flow_key = (pkt[IP].src, pkt[IP].dst, pkt[UDP].sport, pkt[UDP].dport,pkt[UDP].name)\n",
    "                    udp_flows[flow_key].append(pkt)\n",
    "\n",
    "    for key, flow in tcp_flows.items():\n",
    "        flow.sort(key=lambda pkt: pkt.time)\n",
    "        tcp_flows[key] = flow[:threshold]\n",
    "\n",
    "    for key, flow in udp_flows.items():\n",
    "        flow.sort(key=lambda pkt: pkt.time)\n",
    "        udp_flows[key] = flow[:threshold]\n",
    "\n",
    "    return tcp_flows, udp_flows\n",
    "\n",
    "def process_flow(flow, payload_maxlen=1500, exclude_payload=False):\n",
    "    out = []\n",
    "    for packet in flow:\n",
    "        ip_header = mask_ip_header(packet)\n",
    "        tcpudp_header = mask_tcpudp_header(packet)\n",
    "\n",
    "        if TCP in packet:\n",
    "            payload = bytes(packet[TCP].payload)\n",
    "        elif UDP in packet:\n",
    "            payload = bytes(packet[UDP].payload)\n",
    "        else:\n",
    "            raise ValueError\n",
    "        assert len(payload) > 0\n",
    "        payload = list(payload)\n",
    "        payload = payload[:min(payload_maxlen, len(payload))]\n",
    "\n",
    "        out.append([list(ip_header), list(tcpudp_header), payload])\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_flow_dataset(dataset_dir, out_dir, exclude_payload=False, flow_threshold=10):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    DNS.dissect = lambda self, s: None\n",
    "    for root, dirs, files in os.walk(dataset_dir):  # subdir level\n",
    "        for file in files: \n",
    "            if file.endswith('.pcap'):\n",
    "                file_path = os.path.join(root, file)  \n",
    "                label = file_path.split('/')[-2]\n",
    "                \n",
    "                print(file_path)\n",
    "                packets = rdpcap(file_path)\n",
    "                tcp_flows, udp_flows = extract_flows(packets) #list of flows\n",
    "\n",
    "                flows = []\n",
    "                out_file = os.path.join(out_dir, label+'.pkl')\n",
    "\n",
    "                if len(tcp_flows) > 0:\n",
    "                    for flow in tcp_flows.values():\n",
    "                        if len(flow) > flow_threshold:\n",
    "                            flows.append(process_flow(flow))\n",
    "                if len(udp_flows) > 0:\n",
    "                    for flow in udp_flows.values():\n",
    "                        if len(flow) > flow_threshold:\n",
    "                            flows.append(process_flow(flow))\n",
    "\n",
    "                with open(out_file, \"ab\") as f: \n",
    "                    pickle.dump(flows, f)\n",
    "                print(f'Extracted {len(flows)} flows out of {len(packets)} packets')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = '../data/raw/D2'\n",
    "out_dir = '../data/processed/D2_flow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/raw/D2/twitter/twitter_1.pcap\n",
      "Extracted 137 flows out of 356192 packets\n",
      "../data/raw/D2/ted/TED_2.pcap\n",
      "Extracted 65 flows out of 78244 packets\n",
      "../data/raw/D2/ted/TED_1.pcap\n",
      "Extracted 44 flows out of 353894 packets\n",
      "../data/raw/D2/amazon/amazon_1.pcap\n",
      "Extracted 277 flows out of 250460 packets\n",
      "../data/raw/D2/baidu/baidu_2.pcap\n",
      "Extracted 123 flows out of 23688 packets\n",
      "../data/raw/D2/baidu/baidu_1.pcap\n",
      "Extracted 164 flows out of 294552 packets\n",
      "../data/raw/D2/youku/youku_1.pcap\n",
      "Extracted 435 flows out of 221172 packets\n",
      "../data/raw/D2/douban/douban_1.pcap\n",
      "Extracted 223 flows out of 215897 packets\n",
      "../data/raw/D2/google/google_2.pcap\n",
      "Extracted 38 flows out of 20460 packets\n",
      "../data/raw/D2/google/google_1.pcap\n",
      "Extracted 189 flows out of 112995 packets\n",
      "../data/raw/D2/bing/bing_2.pcap\n",
      "Extracted 24 flows out of 55395 packets\n",
      "../data/raw/D2/bing/bing_1.pcap\n",
      "Extracted 88 flows out of 200663 packets\n",
      "../data/raw/D2/youtube/youtube_1.pcap\n",
      "Extracted 279 flows out of 519091 packets\n",
      "../data/raw/D2/facebook/facebook_1.pcap\n",
      "Extracted 229 flows out of 369361 packets\n",
      "../data/raw/D2/facebook/facebook_2.pcap\n",
      "Extracted 75 flows out of 65436 packets\n",
      "../data/raw/D2/weibo/weibo_1.pcap\n"
     ]
    }
   ],
   "source": [
    "process_flow_dataset(dataset_dir, out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('../data/processed/D1_flow/tudou_tcp.pkl', \"rb\") as f: \n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "452"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "compress_dir = '../data/processed/D1_flow'\n",
    "out_name = '../data/processed/D1_flow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/tianh/Desktop/EBSNN/data/processed/D1_flow.tar.gz'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "shutil.make_archive(out_name, 'gztar', compress_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
